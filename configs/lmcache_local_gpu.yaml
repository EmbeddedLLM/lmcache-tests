chunk_size: 256
local_device: "cuda"
remote_url: null

# Whether retrieve() is pipelined or not
pipelined_backend: False

# Whether saving decode KV cache to boost multi-turn conversation or not
save_decode_cache: True
